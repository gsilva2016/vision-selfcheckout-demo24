--- autobackend.py	2024-03-19 17:32:44.684851269 +0000
+++ /savedir/autobackend_npu.py	2024-03-19 18:33:35.852782373 +0000
@@ -199,19 +199,19 @@
             w = Path(w)
             if not w.is_file():  # if not *.xml
                 w = next(w.glob("*.xml"))  # get *.xml file from *_openvino_model dir
-            ov_model = core.read_model(model=str(w), weights=w.with_suffix(".bin"))
-            if ov_model.get_parameters()[0].get_layout().empty:
-                ov_model.get_parameters()[0].set_layout(ov.Layout("NCHW"))
-            batch_dim = ov.get_batch(ov_model)
-            if batch_dim.is_static:
-                batch_size = batch_dim.get_length()
+            model_quant_xml = "/savedir/yolov8n-int8.xml"
+            model_quant_bin = "/savedir/yolov8n-int8.bin"
+            ov_model = core.read_model(model=model_quant_xml, weights=model_quant_bin)
+            ov_model.reshape({0: [1,3, 384,640]})
+            batch_size =1
+            batch = 1
 
             # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'
             inference_mode = "CUMULATIVE_THROUGHPUT" if batch > 1 else "LATENCY"
             LOGGER.info(f"Using OpenVINO {inference_mode} mode for batch-size={batch_size} inference...")
             ov_compiled_model = core.compile_model(
                 ov_model,
-                device_name="NPU",  # AUTO selects best available device, do not modify
+                device_name="NPU",
                 config={"PERFORMANCE_HINT": inference_mode},
             )
             input_name = ov_compiled_model.input().get_any_name()
