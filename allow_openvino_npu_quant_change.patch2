--- autobackend.py	2024-03-19 23:29:05.726491977 +0000
+++ autobackend_npu.py	2024-03-19 23:41:56.415929828 +0000
@@ -195,23 +195,25 @@
             check_requirements("openvino>=2024.0.0")
             import openvino as ov
 
-            core = ov.Core()
             w = Path(w)
             if not w.is_file():  # if not *.xml
                 w = next(w.glob("*.xml"))  # get *.xml file from *_openvino_model dir
-            ov_model = core.read_model(model=str(w), weights=w.with_suffix(".bin"))
-            if ov_model.get_parameters()[0].get_layout().empty:
-                ov_model.get_parameters()[0].set_layout(ov.Layout("NCHW"))
-            batch_dim = ov.get_batch(ov_model)
-            if batch_dim.is_static:
-                batch_size = batch_dim.get_length()
+
+            core = ov.Core()
+            model_quant_xml = "/savedir/yolov8n-int8.xml"
+            model_quant_bin = "/savedir/yolov8n-int8.bin"
+            ov_model = core.read_model(model=model_quant_xml, weights=model_quant_bin)
+            ov_model.reshape({0: [1,3, 384,640]})
+            batch_size =1
+            batch = 1
+
 
             # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'
             inference_mode = "CUMULATIVE_THROUGHPUT" if batch > 1 else "LATENCY"
             LOGGER.info(f"Using OpenVINO {inference_mode} mode for batch-size={batch_size} inference...")
             ov_compiled_model = core.compile_model(
                 ov_model,
-                device_name="GPU",  # AUTO selects best available device, do not modify
+                device_name="NPU",
                 config={"PERFORMANCE_HINT": inference_mode},
             )
             input_name = ov_compiled_model.input().get_any_name()
